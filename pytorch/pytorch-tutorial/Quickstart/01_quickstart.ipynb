{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchは2つのデータセットがある。  \n",
    "torch.utils.data.DataLoader：Datasetのラッパー  \n",
    "torch.utils.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorchにはドメインごとにライブラリが存在する。TorchText, TorchVision, and TorchAudio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FashionMNISTを使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download train datasets\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# download test datasets\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models\n",
    "PyTorchでニューラルネットワークを定義するためには、nn.Moduleを継承したクラスを作成する。レイヤーは```__init__```関数で定義する。データが伝播する方式は```forward```関数で定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizing the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, corrent = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{corrent:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss:{test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.312848 [   64/60000]\n",
      "loss: 2.297966 [  704/60000]\n",
      "loss: 2.298067 [ 1344/60000]\n",
      "loss: 2.301866 [ 1984/60000]\n",
      "loss: 2.305833 [ 2624/60000]\n",
      "loss: 2.303755 [ 3264/60000]\n",
      "loss: 2.292912 [ 3904/60000]\n",
      "loss: 2.287566 [ 4544/60000]\n",
      "loss: 2.299021 [ 5184/60000]\n",
      "loss: 2.299621 [ 5824/60000]\n",
      "loss: 2.297798 [ 6464/60000]\n",
      "loss: 2.289417 [ 7104/60000]\n",
      "loss: 2.284974 [ 7744/60000]\n",
      "loss: 2.284792 [ 8384/60000]\n",
      "loss: 2.286625 [ 9024/60000]\n",
      "loss: 2.279461 [ 9664/60000]\n",
      "loss: 2.287282 [10304/60000]\n",
      "loss: 2.283288 [10944/60000]\n",
      "loss: 2.277294 [11584/60000]\n",
      "loss: 2.277409 [12224/60000]\n",
      "loss: 2.275964 [12864/60000]\n",
      "loss: 2.280407 [13504/60000]\n",
      "loss: 2.272880 [14144/60000]\n",
      "loss: 2.267057 [14784/60000]\n",
      "loss: 2.277332 [15424/60000]\n",
      "loss: 2.278931 [16064/60000]\n",
      "loss: 2.267663 [16704/60000]\n",
      "loss: 2.268663 [17344/60000]\n",
      "loss: 2.273103 [17984/60000]\n",
      "loss: 2.273017 [18624/60000]\n",
      "loss: 2.268409 [19264/60000]\n",
      "loss: 2.260637 [19904/60000]\n",
      "loss: 2.267769 [20544/60000]\n",
      "loss: 2.251122 [21184/60000]\n",
      "loss: 2.271024 [21824/60000]\n",
      "loss: 2.251535 [22464/60000]\n",
      "loss: 2.264600 [23104/60000]\n",
      "loss: 2.257600 [23744/60000]\n",
      "loss: 2.265250 [24384/60000]\n",
      "loss: 2.251493 [25024/60000]\n",
      "loss: 2.250287 [25664/60000]\n",
      "loss: 2.250766 [26304/60000]\n",
      "loss: 2.235409 [26944/60000]\n",
      "loss: 2.246858 [27584/60000]\n",
      "loss: 2.251118 [28224/60000]\n",
      "loss: 2.238593 [28864/60000]\n",
      "loss: 2.266641 [29504/60000]\n",
      "loss: 2.242468 [30144/60000]\n",
      "loss: 2.243019 [30784/60000]\n",
      "loss: 2.248258 [31424/60000]\n",
      "loss: 2.235021 [32064/60000]\n",
      "loss: 2.225367 [32704/60000]\n",
      "loss: 2.239098 [33344/60000]\n",
      "loss: 2.239211 [33984/60000]\n",
      "loss: 2.234815 [34624/60000]\n",
      "loss: 2.210095 [35264/60000]\n",
      "loss: 2.225958 [35904/60000]\n",
      "loss: 2.228775 [36544/60000]\n",
      "loss: 2.242289 [37184/60000]\n",
      "loss: 2.235780 [37824/60000]\n",
      "loss: 2.240388 [38464/60000]\n",
      "loss: 2.224332 [39104/60000]\n",
      "loss: 2.230273 [39744/60000]\n",
      "loss: 2.216025 [40384/60000]\n",
      "loss: 2.231784 [41024/60000]\n",
      "loss: 2.212367 [41664/60000]\n",
      "loss: 2.213994 [42304/60000]\n",
      "loss: 2.220353 [42944/60000]\n",
      "loss: 2.210586 [43584/60000]\n",
      "loss: 2.196897 [44224/60000]\n",
      "loss: 2.212253 [44864/60000]\n",
      "loss: 2.211219 [45504/60000]\n",
      "loss: 2.231985 [46144/60000]\n",
      "loss: 2.252007 [46784/60000]\n",
      "loss: 2.208833 [47424/60000]\n",
      "loss: 2.199045 [48064/60000]\n",
      "loss: 2.183573 [48704/60000]\n",
      "loss: 2.213136 [49344/60000]\n",
      "loss: 2.224109 [49984/60000]\n",
      "loss: 2.209302 [50624/60000]\n",
      "loss: 2.215837 [51264/60000]\n",
      "loss: 2.182410 [51904/60000]\n",
      "loss: 2.211845 [52544/60000]\n",
      "loss: 2.193346 [53184/60000]\n",
      "loss: 2.181060 [53824/60000]\n",
      "loss: 2.188748 [54464/60000]\n",
      "loss: 2.198073 [55104/60000]\n",
      "loss: 2.178151 [55744/60000]\n",
      "loss: 2.174549 [56384/60000]\n",
      "loss: 2.175438 [57024/60000]\n",
      "loss: 2.183193 [57664/60000]\n",
      "loss: 2.174511 [58304/60000]\n",
      "loss: 2.186613 [58944/60000]\n",
      "loss: 2.167635 [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.3%, Avg loss:2.174892 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.195209 [   64/60000]\n",
      "loss: 2.161678 [  704/60000]\n",
      "loss: 2.161903 [ 1344/60000]\n",
      "loss: 2.170691 [ 1984/60000]\n",
      "loss: 2.167627 [ 2624/60000]\n",
      "loss: 2.188374 [ 3264/60000]\n",
      "loss: 2.159514 [ 3904/60000]\n",
      "loss: 2.138186 [ 4544/60000]\n",
      "loss: 2.160927 [ 5184/60000]\n",
      "loss: 2.180263 [ 5824/60000]\n",
      "loss: 2.176892 [ 6464/60000]\n",
      "loss: 2.158086 [ 7104/60000]\n",
      "loss: 2.149054 [ 7744/60000]\n",
      "loss: 2.132101 [ 8384/60000]\n",
      "loss: 2.143156 [ 9024/60000]\n",
      "loss: 2.152506 [ 9664/60000]\n",
      "loss: 2.165146 [10304/60000]\n",
      "loss: 2.139005 [10944/60000]\n",
      "loss: 2.147193 [11584/60000]\n",
      "loss: 2.123364 [12224/60000]\n",
      "loss: 2.125891 [12864/60000]\n",
      "loss: 2.148202 [13504/60000]\n",
      "loss: 2.138700 [14144/60000]\n",
      "loss: 2.095617 [14784/60000]\n",
      "loss: 2.145915 [15424/60000]\n",
      "loss: 2.144941 [16064/60000]\n",
      "loss: 2.105414 [16704/60000]\n",
      "loss: 2.111925 [17344/60000]\n",
      "loss: 2.126801 [17984/60000]\n",
      "loss: 2.120060 [18624/60000]\n",
      "loss: 2.139011 [19264/60000]\n",
      "loss: 2.113462 [19904/60000]\n",
      "loss: 2.104100 [20544/60000]\n",
      "loss: 2.069422 [21184/60000]\n",
      "loss: 2.121565 [21824/60000]\n",
      "loss: 2.081600 [22464/60000]\n",
      "loss: 2.121950 [23104/60000]\n",
      "loss: 2.102450 [23744/60000]\n",
      "loss: 2.127101 [24384/60000]\n",
      "loss: 2.088338 [25024/60000]\n",
      "loss: 2.083497 [25664/60000]\n",
      "loss: 2.085829 [26304/60000]\n",
      "loss: 2.041006 [26944/60000]\n",
      "loss: 2.097422 [27584/60000]\n",
      "loss: 2.087367 [28224/60000]\n",
      "loss: 2.058223 [28864/60000]\n",
      "loss: 2.119475 [29504/60000]\n",
      "loss: 2.053650 [30144/60000]\n",
      "loss: 2.086210 [30784/60000]\n",
      "loss: 2.078195 [31424/60000]\n",
      "loss: 2.044632 [32064/60000]\n",
      "loss: 2.015493 [32704/60000]\n",
      "loss: 2.058139 [33344/60000]\n",
      "loss: 2.071279 [33984/60000]\n",
      "loss: 2.058394 [34624/60000]\n",
      "loss: 1.992583 [35264/60000]\n",
      "loss: 2.029477 [35904/60000]\n",
      "loss: 2.026784 [36544/60000]\n",
      "loss: 2.079342 [37184/60000]\n",
      "loss: 2.054598 [37824/60000]\n",
      "loss: 2.067737 [38464/60000]\n",
      "loss: 2.016135 [39104/60000]\n",
      "loss: 2.047750 [39744/60000]\n",
      "loss: 2.009580 [40384/60000]\n",
      "loss: 2.055108 [41024/60000]\n",
      "loss: 2.006408 [41664/60000]\n",
      "loss: 2.008971 [42304/60000]\n",
      "loss: 2.032845 [42944/60000]\n",
      "loss: 2.003576 [43584/60000]\n",
      "loss: 1.968499 [44224/60000]\n",
      "loss: 1.998073 [44864/60000]\n",
      "loss: 2.011670 [45504/60000]\n",
      "loss: 2.053486 [46144/60000]\n",
      "loss: 2.101454 [46784/60000]\n",
      "loss: 2.001796 [47424/60000]\n",
      "loss: 1.972941 [48064/60000]\n",
      "loss: 1.946327 [48704/60000]\n",
      "loss: 2.007336 [49344/60000]\n",
      "loss: 2.025150 [49984/60000]\n",
      "loss: 2.010369 [50624/60000]\n",
      "loss: 2.010169 [51264/60000]\n",
      "loss: 1.943029 [51904/60000]\n",
      "loss: 2.010745 [52544/60000]\n",
      "loss: 1.964521 [53184/60000]\n",
      "loss: 1.945289 [53824/60000]\n",
      "loss: 1.959013 [54464/60000]\n",
      "loss: 1.973784 [55104/60000]\n",
      "loss: 1.928592 [55744/60000]\n",
      "loss: 1.937737 [56384/60000]\n",
      "loss: 1.939541 [57024/60000]\n",
      "loss: 1.941306 [57664/60000]\n",
      "loss: 1.939324 [58304/60000]\n",
      "loss: 1.963218 [58944/60000]\n",
      "loss: 1.923617 [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss:1.931981 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.972700 [   64/60000]\n",
      "loss: 1.891901 [  704/60000]\n",
      "loss: 1.909198 [ 1344/60000]\n",
      "loss: 1.929775 [ 1984/60000]\n",
      "loss: 1.911113 [ 2624/60000]\n",
      "loss: 1.959401 [ 3264/60000]\n",
      "loss: 1.908566 [ 3904/60000]\n",
      "loss: 1.868893 [ 4544/60000]\n",
      "loss: 1.905604 [ 5184/60000]\n",
      "loss: 1.944647 [ 5824/60000]\n",
      "loss: 1.935605 [ 6464/60000]\n",
      "loss: 1.882715 [ 7104/60000]\n",
      "loss: 1.873178 [ 7744/60000]\n",
      "loss: 1.857499 [ 8384/60000]\n",
      "loss: 1.869879 [ 9024/60000]\n",
      "loss: 1.909362 [ 9664/60000]\n",
      "loss: 1.933188 [10304/60000]\n",
      "loss: 1.875024 [10944/60000]\n",
      "loss: 1.885994 [11584/60000]\n",
      "loss: 1.836700 [12224/60000]\n",
      "loss: 1.827709 [12864/60000]\n",
      "loss: 1.887745 [13504/60000]\n",
      "loss: 1.882533 [14144/60000]\n",
      "loss: 1.778496 [14784/60000]\n",
      "loss: 1.867727 [15424/60000]\n",
      "loss: 1.893405 [16064/60000]\n",
      "loss: 1.789495 [16704/60000]\n",
      "loss: 1.822106 [17344/60000]\n",
      "loss: 1.842425 [17984/60000]\n",
      "loss: 1.828207 [18624/60000]\n",
      "loss: 1.864684 [19264/60000]\n",
      "loss: 1.831990 [19904/60000]\n",
      "loss: 1.758973 [20544/60000]\n",
      "loss: 1.728750 [21184/60000]\n",
      "loss: 1.824398 [21824/60000]\n",
      "loss: 1.748306 [22464/60000]\n",
      "loss: 1.854312 [23104/60000]\n",
      "loss: 1.801701 [23744/60000]\n",
      "loss: 1.853900 [24384/60000]\n",
      "loss: 1.775990 [25024/60000]\n",
      "loss: 1.748171 [25664/60000]\n",
      "loss: 1.771613 [26304/60000]\n",
      "loss: 1.664955 [26944/60000]\n",
      "loss: 1.792183 [27584/60000]\n",
      "loss: 1.760235 [28224/60000]\n",
      "loss: 1.743106 [28864/60000]\n",
      "loss: 1.836507 [29504/60000]\n",
      "loss: 1.713283 [30144/60000]\n",
      "loss: 1.764384 [30784/60000]\n",
      "loss: 1.739748 [31424/60000]\n",
      "loss: 1.706257 [32064/60000]\n",
      "loss: 1.625534 [32704/60000]\n",
      "loss: 1.703599 [33344/60000]\n",
      "loss: 1.763994 [33984/60000]\n",
      "loss: 1.736729 [34624/60000]\n",
      "loss: 1.597870 [35264/60000]\n",
      "loss: 1.672416 [35904/60000]\n",
      "loss: 1.681412 [36544/60000]\n",
      "loss: 1.752235 [37184/60000]\n",
      "loss: 1.718355 [37824/60000]\n",
      "loss: 1.729406 [38464/60000]\n",
      "loss: 1.638237 [39104/60000]\n",
      "loss: 1.717069 [39744/60000]\n",
      "loss: 1.637392 [40384/60000]\n",
      "loss: 1.729643 [41024/60000]\n",
      "loss: 1.651937 [41664/60000]\n",
      "loss: 1.658697 [42304/60000]\n",
      "loss: 1.706135 [42944/60000]\n",
      "loss: 1.644301 [43584/60000]\n",
      "loss: 1.583605 [44224/60000]\n",
      "loss: 1.629134 [44864/60000]\n",
      "loss: 1.646292 [45504/60000]\n",
      "loss: 1.718661 [46144/60000]\n",
      "loss: 1.797575 [46784/60000]\n",
      "loss: 1.656682 [47424/60000]\n",
      "loss: 1.581813 [48064/60000]\n",
      "loss: 1.584431 [48704/60000]\n",
      "loss: 1.651662 [49344/60000]\n",
      "loss: 1.677228 [49984/60000]\n",
      "loss: 1.684967 [50624/60000]\n",
      "loss: 1.654216 [51264/60000]\n",
      "loss: 1.566339 [51904/60000]\n",
      "loss: 1.665368 [52544/60000]\n",
      "loss: 1.583899 [53184/60000]\n",
      "loss: 1.574204 [53824/60000]\n",
      "loss: 1.588295 [54464/60000]\n",
      "loss: 1.591951 [55104/60000]\n",
      "loss: 1.530131 [55744/60000]\n",
      "loss: 1.573262 [56384/60000]\n",
      "loss: 1.569016 [57024/60000]\n",
      "loss: 1.548181 [57664/60000]\n",
      "loss: 1.576398 [58304/60000]\n",
      "loss: 1.618222 [58944/60000]\n",
      "loss: 1.554689 [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss:1.559702 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.629862 [   64/60000]\n",
      "loss: 1.467928 [  704/60000]\n",
      "loss: 1.545914 [ 1344/60000]\n",
      "loss: 1.553859 [ 1984/60000]\n",
      "loss: 1.530974 [ 2624/60000]\n",
      "loss: 1.597483 [ 3264/60000]\n",
      "loss: 1.540127 [ 3904/60000]\n",
      "loss: 1.498513 [ 4544/60000]\n",
      "loss: 1.542624 [ 5184/60000]\n",
      "loss: 1.587057 [ 5824/60000]\n",
      "loss: 1.586449 [ 6464/60000]\n",
      "loss: 1.487686 [ 7104/60000]\n",
      "loss: 1.491462 [ 7744/60000]\n",
      "loss: 1.496186 [ 8384/60000]\n",
      "loss: 1.496998 [ 9024/60000]\n",
      "loss: 1.574131 [ 9664/60000]\n",
      "loss: 1.629048 [10304/60000]\n",
      "loss: 1.520850 [10944/60000]\n",
      "loss: 1.540693 [11584/60000]\n",
      "loss: 1.470933 [12224/60000]\n",
      "loss: 1.440277 [12864/60000]\n",
      "loss: 1.552229 [13504/60000]\n",
      "loss: 1.540854 [14144/60000]\n",
      "loss: 1.386569 [14784/60000]\n",
      "loss: 1.508866 [15424/60000]\n",
      "loss: 1.561226 [16064/60000]\n",
      "loss: 1.380013 [16704/60000]\n",
      "loss: 1.467643 [17344/60000]\n",
      "loss: 1.485192 [17984/60000]\n",
      "loss: 1.469704 [18624/60000]\n",
      "loss: 1.511249 [19264/60000]\n",
      "loss: 1.489068 [19904/60000]\n",
      "loss: 1.337500 [20544/60000]\n",
      "loss: 1.334108 [21184/60000]\n",
      "loss: 1.452553 [21824/60000]\n",
      "loss: 1.351138 [22464/60000]\n",
      "loss: 1.541883 [23104/60000]\n",
      "loss: 1.436010 [23744/60000]\n",
      "loss: 1.527598 [24384/60000]\n",
      "loss: 1.417242 [25024/60000]\n",
      "loss: 1.379000 [25664/60000]\n",
      "loss: 1.421355 [26304/60000]\n",
      "loss: 1.273451 [26944/60000]\n",
      "loss: 1.445367 [27584/60000]\n",
      "loss: 1.392037 [28224/60000]\n",
      "loss: 1.407793 [28864/60000]\n",
      "loss: 1.515156 [29504/60000]\n",
      "loss: 1.364319 [30144/60000]\n",
      "loss: 1.424536 [30784/60000]\n",
      "loss: 1.375975 [31424/60000]\n",
      "loss: 1.375551 [32064/60000]\n",
      "loss: 1.258190 [32704/60000]\n",
      "loss: 1.329111 [33344/60000]\n",
      "loss: 1.443243 [33984/60000]\n",
      "loss: 1.405444 [34624/60000]\n",
      "loss: 1.237790 [35264/60000]\n",
      "loss: 1.322429 [35904/60000]\n",
      "loss: 1.337096 [36544/60000]\n",
      "loss: 1.403124 [37184/60000]\n",
      "loss: 1.374218 [37824/60000]\n",
      "loss: 1.390105 [38464/60000]\n",
      "loss: 1.282351 [39104/60000]\n",
      "loss: 1.398234 [39744/60000]\n",
      "loss: 1.282279 [40384/60000]\n",
      "loss: 1.410759 [41024/60000]\n",
      "loss: 1.331226 [41664/60000]\n",
      "loss: 1.344115 [42304/60000]\n",
      "loss: 1.392437 [42944/60000]\n",
      "loss: 1.311687 [43584/60000]\n",
      "loss: 1.259599 [44224/60000]\n",
      "loss: 1.315059 [44864/60000]\n",
      "loss: 1.292950 [45504/60000]\n",
      "loss: 1.423126 [46144/60000]\n",
      "loss: 1.478268 [46784/60000]\n",
      "loss: 1.336273 [47424/60000]\n",
      "loss: 1.229106 [48064/60000]\n",
      "loss: 1.308018 [48704/60000]\n",
      "loss: 1.335439 [49344/60000]\n",
      "loss: 1.375919 [49984/60000]\n",
      "loss: 1.390663 [50624/60000]\n",
      "loss: 1.346572 [51264/60000]\n",
      "loss: 1.273574 [51904/60000]\n",
      "loss: 1.346040 [52544/60000]\n",
      "loss: 1.278350 [53184/60000]\n",
      "loss: 1.257476 [53824/60000]\n",
      "loss: 1.289377 [54464/60000]\n",
      "loss: 1.278710 [55104/60000]\n",
      "loss: 1.237796 [55744/60000]\n",
      "loss: 1.276691 [56384/60000]\n",
      "loss: 1.284496 [57024/60000]\n",
      "loss: 1.243569 [57664/60000]\n",
      "loss: 1.270308 [58304/60000]\n",
      "loss: 1.350648 [58944/60000]\n",
      "loss: 1.255279 [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss:1.270930 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.353522 [   64/60000]\n",
      "loss: 1.142377 [  704/60000]\n",
      "loss: 1.263936 [ 1344/60000]\n",
      "loss: 1.246700 [ 1984/60000]\n",
      "loss: 1.240487 [ 2624/60000]\n",
      "loss: 1.308245 [ 3264/60000]\n",
      "loss: 1.275143 [ 3904/60000]\n",
      "loss: 1.225196 [ 4544/60000]\n",
      "loss: 1.276067 [ 5184/60000]\n",
      "loss: 1.298224 [ 5824/60000]\n",
      "loss: 1.327737 [ 6464/60000]\n",
      "loss: 1.230581 [ 7104/60000]\n",
      "loss: 1.227306 [ 7744/60000]\n",
      "loss: 1.243178 [ 8384/60000]\n",
      "loss: 1.243061 [ 9024/60000]\n",
      "loss: 1.342744 [ 9664/60000]\n",
      "loss: 1.411670 [10304/60000]\n",
      "loss: 1.262888 [10944/60000]\n",
      "loss: 1.303146 [11584/60000]\n",
      "loss: 1.212255 [12224/60000]\n",
      "loss: 1.164465 [12864/60000]\n",
      "loss: 1.316891 [13504/60000]\n",
      "loss: 1.284092 [14144/60000]\n",
      "loss: 1.121629 [14784/60000]\n",
      "loss: 1.270087 [15424/60000]\n",
      "loss: 1.311828 [16064/60000]\n",
      "loss: 1.100558 [16704/60000]\n",
      "loss: 1.224571 [17344/60000]\n",
      "loss: 1.231072 [17984/60000]\n",
      "loss: 1.231549 [18624/60000]\n",
      "loss: 1.271251 [19264/60000]\n",
      "loss: 1.250437 [19904/60000]\n",
      "loss: 1.068336 [20544/60000]\n",
      "loss: 1.067451 [21184/60000]\n",
      "loss: 1.207686 [21824/60000]\n",
      "loss: 1.080601 [22464/60000]\n",
      "loss: 1.324473 [23104/60000]\n",
      "loss: 1.180942 [23744/60000]\n",
      "loss: 1.304468 [24384/60000]\n",
      "loss: 1.177692 [25024/60000]\n",
      "loss: 1.137696 [25664/60000]\n",
      "loss: 1.197976 [26304/60000]\n",
      "loss: 1.034517 [26944/60000]\n",
      "loss: 1.220452 [27584/60000]\n",
      "loss: 1.161528 [28224/60000]\n",
      "loss: 1.203359 [28864/60000]\n",
      "loss: 1.298780 [29504/60000]\n",
      "loss: 1.151519 [30144/60000]\n",
      "loss: 1.227258 [30784/60000]\n",
      "loss: 1.134037 [31424/60000]\n",
      "loss: 1.163392 [32064/60000]\n",
      "loss: 1.035764 [32704/60000]\n",
      "loss: 1.083137 [33344/60000]\n",
      "loss: 1.241396 [33984/60000]\n",
      "loss: 1.193361 [34624/60000]\n",
      "loss: 1.012905 [35264/60000]\n",
      "loss: 1.106623 [35904/60000]\n",
      "loss: 1.120078 [36544/60000]\n",
      "loss: 1.174879 [37184/60000]\n",
      "loss: 1.155944 [37824/60000]\n",
      "loss: 1.182904 [38464/60000]\n",
      "loss: 1.068022 [39104/60000]\n",
      "loss: 1.198116 [39744/60000]\n",
      "loss: 1.049076 [40384/60000]\n",
      "loss: 1.198483 [41024/60000]\n",
      "loss: 1.133479 [41664/60000]\n",
      "loss: 1.154639 [42304/60000]\n",
      "loss: 1.189111 [42944/60000]\n",
      "loss: 1.113035 [43584/60000]\n",
      "loss: 1.064851 [44224/60000]\n",
      "loss: 1.123499 [44864/60000]\n",
      "loss: 1.070823 [45504/60000]\n",
      "loss: 1.241298 [46144/60000]\n",
      "loss: 1.271602 [46784/60000]\n",
      "loss: 1.130970 [47424/60000]\n",
      "loss: 1.004788 [48064/60000]\n",
      "loss: 1.141423 [48704/60000]\n",
      "loss: 1.141691 [49344/60000]\n",
      "loss: 1.192178 [49984/60000]\n",
      "loss: 1.206146 [50624/60000]\n",
      "loss: 1.157749 [51264/60000]\n",
      "loss: 1.099299 [51904/60000]\n",
      "loss: 1.141102 [52544/60000]\n",
      "loss: 1.103363 [53184/60000]\n",
      "loss: 1.053893 [53824/60000]\n",
      "loss: 1.118861 [54464/60000]\n",
      "loss: 1.085940 [55104/60000]\n",
      "loss: 1.058391 [55744/60000]\n",
      "loss: 1.094010 [56384/60000]\n",
      "loss: 1.118623 [57024/60000]\n",
      "loss: 1.073392 [57664/60000]\n",
      "loss: 1.067700 [58304/60000]\n",
      "loss: 1.189647 [58944/60000]\n",
      "loss: 1.078373 [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss:1.095808 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0146, -0.0073,  0.0150,  ...,  0.0017, -0.0130,  0.0177],\n",
       "                      [ 0.0314, -0.0279, -0.0003,  ..., -0.0083,  0.0152, -0.0196],\n",
       "                      [ 0.0186,  0.0092, -0.0058,  ...,  0.0276, -0.0184,  0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0122, -0.0065,  0.0026,  ...,  0.0119, -0.0122, -0.0115],\n",
       "                      [ 0.0282,  0.0267,  0.0270,  ..., -0.0250,  0.0276,  0.0245],\n",
       "                      [-0.0321, -0.0226,  0.0324,  ..., -0.0057, -0.0308, -0.0293]],\n",
       "                     device='mps:0')),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 0.0156,  0.0418,  0.0314,  0.0044,  0.0359, -0.0142,  0.0208, -0.0014,\n",
       "                      -0.0018, -0.0336, -0.0100, -0.0079,  0.0127,  0.0019,  0.0253, -0.0168,\n",
       "                       0.0368,  0.0111,  0.0202, -0.0266, -0.0197,  0.0079, -0.0307,  0.0315,\n",
       "                      -0.0233,  0.0217,  0.0121,  0.0423, -0.0259,  0.0330,  0.0138, -0.0203,\n",
       "                      -0.0127,  0.0321,  0.0289, -0.0028,  0.0264,  0.0164,  0.0240, -0.0166,\n",
       "                       0.0345,  0.0390,  0.0116,  0.0248, -0.0292,  0.0217,  0.0187,  0.0053,\n",
       "                       0.0317, -0.0247, -0.0283, -0.0267, -0.0043,  0.0142, -0.0184,  0.0329,\n",
       "                       0.0218,  0.0243,  0.0199,  0.0370,  0.0336, -0.0175,  0.0257, -0.0184,\n",
       "                       0.0365,  0.0351,  0.0249,  0.0292,  0.0208,  0.0083,  0.0086, -0.0204,\n",
       "                      -0.0017,  0.0327,  0.0139,  0.0373, -0.0318,  0.0053,  0.0335,  0.0180,\n",
       "                      -0.0114,  0.0346, -0.0026,  0.0346, -0.0073,  0.0394, -0.0102,  0.0114,\n",
       "                      -0.0140, -0.0092, -0.0154,  0.0287,  0.0186,  0.0001,  0.0012, -0.0102,\n",
       "                      -0.0038, -0.0031,  0.0056, -0.0136,  0.0194,  0.0331, -0.0219,  0.0037,\n",
       "                      -0.0228, -0.0247,  0.0286, -0.0003, -0.0234,  0.0173,  0.0210, -0.0062,\n",
       "                       0.0343,  0.0255,  0.0197,  0.0288, -0.0152, -0.0128, -0.0052,  0.0003,\n",
       "                      -0.0119, -0.0027,  0.0336, -0.0166,  0.0288,  0.0390, -0.0091, -0.0143,\n",
       "                      -0.0188, -0.0146,  0.0310,  0.0036,  0.0037, -0.0305, -0.0202, -0.0196,\n",
       "                      -0.0035, -0.0040, -0.0009, -0.0025,  0.0221,  0.0248, -0.0083,  0.0280,\n",
       "                      -0.0119,  0.0285, -0.0145,  0.0340,  0.0154,  0.0270,  0.0282,  0.0096,\n",
       "                       0.0216,  0.0394,  0.0112,  0.0071,  0.0009,  0.0237,  0.0384,  0.0337,\n",
       "                      -0.0250,  0.0020,  0.0079, -0.0010,  0.0212, -0.0024, -0.0063,  0.0077,\n",
       "                      -0.0230,  0.0367,  0.0254,  0.0106, -0.0004,  0.0012,  0.0321, -0.0056,\n",
       "                       0.0186,  0.0065,  0.0232,  0.0242, -0.0327, -0.0003, -0.0174, -0.0333,\n",
       "                      -0.0153, -0.0280,  0.0084,  0.0057, -0.0221, -0.0288, -0.0250,  0.0303,\n",
       "                      -0.0018, -0.0116,  0.0097,  0.0221,  0.0218,  0.0296,  0.0083, -0.0060,\n",
       "                      -0.0184,  0.0233, -0.0190, -0.0146, -0.0028,  0.0326,  0.0102,  0.0171,\n",
       "                       0.0103,  0.0233,  0.0001, -0.0190, -0.0151, -0.0306,  0.0227,  0.0169,\n",
       "                       0.0018,  0.0099,  0.0306, -0.0283,  0.0149,  0.0137, -0.0167, -0.0065,\n",
       "                      -0.0021,  0.0087, -0.0326, -0.0248,  0.0180, -0.0290, -0.0054,  0.0391,\n",
       "                       0.0228, -0.0321,  0.0344,  0.0227,  0.0341,  0.0186,  0.0100, -0.0081,\n",
       "                       0.0360, -0.0049,  0.0391, -0.0338, -0.0302, -0.0367,  0.0244,  0.0255,\n",
       "                      -0.0166,  0.0265,  0.0260, -0.0133, -0.0192,  0.0388, -0.0280,  0.0268,\n",
       "                      -0.0159,  0.0436,  0.0244,  0.0344,  0.0163, -0.0046, -0.0152, -0.0058,\n",
       "                       0.0318,  0.0279, -0.0344,  0.0304, -0.0156, -0.0322, -0.0214,  0.0315,\n",
       "                      -0.0133,  0.0281, -0.0213, -0.0206,  0.0155,  0.0013, -0.0113,  0.0093,\n",
       "                      -0.0169,  0.0215,  0.0199,  0.0308,  0.0240,  0.0074,  0.0378,  0.0078,\n",
       "                       0.0325,  0.0226,  0.0018,  0.0067,  0.0052,  0.0014,  0.0233, -0.0329,\n",
       "                      -0.0063, -0.0020, -0.0260, -0.0308,  0.0366, -0.0128, -0.0255,  0.0330,\n",
       "                       0.0214, -0.0220, -0.0214, -0.0295, -0.0202,  0.0057, -0.0128,  0.0091,\n",
       "                      -0.0299,  0.0071, -0.0116, -0.0353,  0.0122,  0.0098, -0.0290, -0.0141,\n",
       "                      -0.0088,  0.0201,  0.0211, -0.0203,  0.0162,  0.0231,  0.0194,  0.0123,\n",
       "                       0.0260, -0.0121, -0.0259,  0.0314,  0.0199, -0.0145, -0.0271,  0.0092,\n",
       "                       0.0097,  0.0035, -0.0161, -0.0255,  0.0187, -0.0037, -0.0025,  0.0156,\n",
       "                      -0.0276, -0.0289,  0.0218, -0.0229,  0.0085, -0.0223, -0.0199, -0.0363,\n",
       "                       0.0081,  0.0048,  0.0296,  0.0040,  0.0157, -0.0272,  0.0227,  0.0075,\n",
       "                      -0.0171,  0.0249,  0.0350,  0.0374,  0.0074, -0.0286, -0.0001, -0.0342,\n",
       "                      -0.0019, -0.0004, -0.0277, -0.0131, -0.0020,  0.0325,  0.0290, -0.0036,\n",
       "                      -0.0290, -0.0177, -0.0157, -0.0181,  0.0026,  0.0218, -0.0135,  0.0113,\n",
       "                      -0.0259, -0.0299, -0.0247,  0.0025, -0.0003,  0.0122, -0.0161, -0.0258,\n",
       "                      -0.0322,  0.0114, -0.0258,  0.0234, -0.0072,  0.0314,  0.0379, -0.0257,\n",
       "                       0.0357, -0.0207,  0.0394,  0.0289,  0.0130,  0.0079,  0.0213,  0.0283,\n",
       "                       0.0234,  0.0199, -0.0118,  0.0198,  0.0295, -0.0067,  0.0124,  0.0012,\n",
       "                      -0.0050, -0.0106, -0.0027, -0.0126, -0.0023,  0.0231, -0.0280, -0.0028,\n",
       "                      -0.0133,  0.0256,  0.0284, -0.0150,  0.0264,  0.0244,  0.0332,  0.0473,\n",
       "                       0.0066, -0.0338, -0.0034, -0.0322,  0.0120,  0.0090,  0.0335,  0.0259,\n",
       "                       0.0312, -0.0064, -0.0319,  0.0215,  0.0115, -0.0028,  0.0059, -0.0091,\n",
       "                      -0.0049, -0.0247, -0.0066, -0.0021,  0.0137, -0.0193,  0.0158,  0.0333,\n",
       "                       0.0356, -0.0053,  0.0233, -0.0181,  0.0120,  0.0044,  0.0298,  0.0358,\n",
       "                      -0.0324, -0.0181,  0.0450,  0.0020,  0.0186,  0.0116, -0.0029,  0.0169,\n",
       "                      -0.0203, -0.0200,  0.0092,  0.0281,  0.0007,  0.0313,  0.0145,  0.0102,\n",
       "                       0.0277, -0.0197, -0.0319, -0.0126,  0.0369, -0.0059, -0.0070,  0.0029,\n",
       "                       0.0277,  0.0051,  0.0006, -0.0151, -0.0130, -0.0144, -0.0124, -0.0294,\n",
       "                       0.0337,  0.0192,  0.0054,  0.0058, -0.0317,  0.0089,  0.0018,  0.0264,\n",
       "                      -0.0293,  0.0289, -0.0093,  0.0259, -0.0281,  0.0250,  0.0198,  0.0181],\n",
       "                     device='mps:0')),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.0072,  0.0412, -0.0123,  ..., -0.0032, -0.0122, -0.0229],\n",
       "                      [-0.0156,  0.0146, -0.0397,  ..., -0.0139,  0.0097,  0.0097],\n",
       "                      [ 0.0091, -0.0138,  0.0199,  ..., -0.0298,  0.0103, -0.0315],\n",
       "                      ...,\n",
       "                      [ 0.0132,  0.0096, -0.0259,  ..., -0.0226,  0.0374, -0.0288],\n",
       "                      [-0.0276, -0.0345,  0.0450,  ...,  0.0417, -0.0321,  0.0248],\n",
       "                      [ 0.0245,  0.0171,  0.0160,  ..., -0.0294, -0.0078, -0.0203]],\n",
       "                     device='mps:0')),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([-0.0129,  0.0142, -0.0418, -0.0151,  0.0343,  0.0392, -0.0069, -0.0308,\n",
       "                      -0.0401, -0.0054, -0.0086,  0.0660, -0.0396, -0.0111,  0.0141,  0.0341,\n",
       "                       0.0017, -0.0214,  0.0078, -0.0066,  0.0324,  0.0432,  0.0268, -0.0395,\n",
       "                      -0.0369,  0.0046,  0.0402,  0.0358, -0.0262,  0.0155, -0.0215,  0.0257,\n",
       "                      -0.0418, -0.0142,  0.0403,  0.0416,  0.0340, -0.0073, -0.0274, -0.0011,\n",
       "                       0.0156, -0.0112,  0.0419, -0.0198,  0.0524, -0.0386, -0.0246, -0.0374,\n",
       "                       0.0433,  0.0225, -0.0325,  0.0282,  0.0031,  0.0209,  0.0261,  0.0111,\n",
       "                      -0.0402, -0.0027,  0.0367,  0.0327, -0.0168,  0.0100, -0.0105,  0.0500,\n",
       "                       0.0435, -0.0206, -0.0249, -0.0336, -0.0394, -0.0026,  0.0306,  0.0083,\n",
       "                      -0.0003, -0.0127,  0.0105, -0.0304,  0.0268,  0.0278,  0.0372,  0.0528,\n",
       "                      -0.0420, -0.0110, -0.0286,  0.0145, -0.0212,  0.0339, -0.0055,  0.0297,\n",
       "                       0.0008,  0.0113,  0.0131,  0.0503,  0.0377,  0.0306,  0.0030, -0.0247,\n",
       "                      -0.0004,  0.0129,  0.0455, -0.0112, -0.0222,  0.0189, -0.0203, -0.0438,\n",
       "                      -0.0114, -0.0478, -0.0303, -0.0132,  0.0188, -0.0225,  0.0211,  0.0231,\n",
       "                       0.0003,  0.0191,  0.0004, -0.0061,  0.0185,  0.0031,  0.0010,  0.0093,\n",
       "                       0.0441, -0.0263,  0.0089, -0.0073,  0.0195,  0.0240,  0.0163, -0.0288,\n",
       "                      -0.0354, -0.0316,  0.0228, -0.0231,  0.0178, -0.0197,  0.0323, -0.0397,\n",
       "                      -0.0194,  0.0166,  0.0092, -0.0087,  0.0211,  0.0269,  0.0431, -0.0177,\n",
       "                      -0.0101, -0.0137,  0.0258,  0.0344,  0.0012,  0.0147,  0.0140,  0.0053,\n",
       "                       0.0432, -0.0457,  0.0443, -0.0168, -0.0270,  0.0252, -0.0401,  0.0507,\n",
       "                      -0.0363, -0.0241, -0.0370,  0.0048, -0.0247,  0.0070,  0.0259,  0.0221,\n",
       "                       0.0335, -0.0044, -0.0443, -0.0192,  0.0278,  0.0032,  0.0394,  0.0281,\n",
       "                      -0.0289, -0.0365, -0.0262,  0.0220,  0.0378, -0.0104,  0.0020, -0.0226,\n",
       "                      -0.0135, -0.0028,  0.0084,  0.0405, -0.0045,  0.0336,  0.0194, -0.0052,\n",
       "                      -0.0099, -0.0064, -0.0324, -0.0336,  0.0380,  0.0222,  0.0315,  0.0454,\n",
       "                       0.0221, -0.0416, -0.0282, -0.0085, -0.0203,  0.0374, -0.0299,  0.0364,\n",
       "                      -0.0308, -0.0222, -0.0328, -0.0046, -0.0151, -0.0160, -0.0344, -0.0053,\n",
       "                      -0.0137,  0.0324, -0.0065, -0.0258,  0.0239, -0.0305, -0.0012, -0.0125,\n",
       "                      -0.0190, -0.0210, -0.0304,  0.0258, -0.0089, -0.0119,  0.0184, -0.0415,\n",
       "                      -0.0313,  0.0174,  0.0035,  0.0039, -0.0124, -0.0352, -0.0048, -0.0364,\n",
       "                      -0.0177,  0.0187,  0.0069, -0.0014,  0.0344,  0.0212,  0.0289,  0.0100,\n",
       "                      -0.0165,  0.0259,  0.0197,  0.0200, -0.0137,  0.0389, -0.0398,  0.0170,\n",
       "                      -0.0294,  0.0212, -0.0195,  0.0274,  0.0183, -0.0116,  0.0338, -0.0264,\n",
       "                       0.0140,  0.0358, -0.0160, -0.0097, -0.0113, -0.0268,  0.0123,  0.0090,\n",
       "                      -0.0219,  0.0314, -0.0285,  0.0285, -0.0364, -0.0221,  0.0504,  0.0320,\n",
       "                      -0.0306,  0.0117,  0.0232, -0.0399, -0.0311, -0.0035,  0.0210,  0.0079,\n",
       "                      -0.0094,  0.0072, -0.0004,  0.0440,  0.0178,  0.0495, -0.0183,  0.0052,\n",
       "                       0.0562,  0.0283,  0.0231, -0.0330, -0.0173, -0.0067, -0.0103,  0.0422,\n",
       "                       0.0088, -0.0306, -0.0216,  0.0208, -0.0209,  0.0192,  0.0301, -0.0092,\n",
       "                       0.0205, -0.0406,  0.0363,  0.0096,  0.0268,  0.0310, -0.0010, -0.0444,\n",
       "                      -0.0135,  0.0257,  0.0205,  0.0202, -0.0401, -0.0176,  0.0379, -0.0070,\n",
       "                       0.0026, -0.0347, -0.0187,  0.0404,  0.0274, -0.0033,  0.0142, -0.0014,\n",
       "                       0.0095,  0.0438,  0.0291,  0.0142, -0.0418,  0.0113,  0.0338, -0.0182,\n",
       "                      -0.0366, -0.0361, -0.0278,  0.0053,  0.0284, -0.0097,  0.0062,  0.0063,\n",
       "                      -0.0364, -0.0108, -0.0339, -0.0150,  0.0307,  0.0029,  0.0326, -0.0383,\n",
       "                       0.0277, -0.0120, -0.0358, -0.0332,  0.0359,  0.0279, -0.0084, -0.0325,\n",
       "                       0.0439,  0.0036, -0.0206, -0.0106, -0.0381,  0.0234, -0.0458, -0.0211,\n",
       "                       0.0360,  0.0103,  0.0388, -0.0230,  0.0328,  0.0180, -0.0041, -0.0010,\n",
       "                       0.0032, -0.0430,  0.0522,  0.0405,  0.0322, -0.0144,  0.0506,  0.0236,\n",
       "                       0.0095, -0.0391, -0.0216, -0.0265,  0.0271,  0.0572,  0.0028,  0.0274,\n",
       "                       0.0290,  0.0382, -0.0205, -0.0031, -0.0170,  0.0242, -0.0213,  0.0206,\n",
       "                      -0.0325,  0.0448,  0.0330,  0.0067,  0.0209, -0.0211,  0.0010, -0.0004,\n",
       "                       0.0102, -0.0047,  0.0271, -0.0217,  0.0392, -0.0176, -0.0257,  0.0257,\n",
       "                      -0.0280,  0.0313, -0.0044,  0.0205, -0.0245,  0.0423, -0.0359, -0.0152,\n",
       "                      -0.0014,  0.0036,  0.0230,  0.0049,  0.0256, -0.0172,  0.0130, -0.0216,\n",
       "                      -0.0078,  0.0259,  0.0093, -0.0150, -0.0100,  0.0248,  0.0405, -0.0368,\n",
       "                       0.0168,  0.0235,  0.0438, -0.0028,  0.0416, -0.0074, -0.0209,  0.0039,\n",
       "                       0.0310, -0.0267,  0.0182, -0.0519,  0.0180,  0.0003,  0.0407, -0.0409,\n",
       "                      -0.0253,  0.0392,  0.0041,  0.0230, -0.0091,  0.0392, -0.0436,  0.0542,\n",
       "                      -0.0344, -0.0396,  0.0217, -0.0162,  0.0097, -0.0080, -0.0181,  0.0241,\n",
       "                      -0.0291, -0.0183, -0.0298, -0.0064, -0.0057,  0.0236, -0.0223, -0.0251,\n",
       "                      -0.0379, -0.0152,  0.0400, -0.0237, -0.0012,  0.0244,  0.0440,  0.0352,\n",
       "                      -0.0159,  0.0438,  0.0112, -0.0083,  0.0090,  0.0477, -0.0066,  0.0232,\n",
       "                      -0.0132, -0.0375,  0.0346, -0.0320,  0.0241,  0.0165,  0.0121, -0.0081],\n",
       "                     device='mps:0')),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[-0.0523,  0.0162, -0.0125,  ...,  0.0222,  0.0429, -0.0453],\n",
       "                      [ 0.0118, -0.0176,  0.0245,  ...,  0.0258,  0.0411, -0.0475],\n",
       "                      [-0.0576, -0.0159,  0.0137,  ...,  0.0116,  0.0112,  0.0262],\n",
       "                      ...,\n",
       "                      [ 0.0709, -0.0146,  0.0247,  ...,  0.0607,  0.0088,  0.0670],\n",
       "                      [ 0.0404,  0.0414,  0.0250,  ..., -0.0404,  0.0287,  0.0651],\n",
       "                      [-0.0302, -0.0158, -0.0044,  ...,  0.0016,  0.0058,  0.0135]],\n",
       "                     device='mps:0')),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.0617,  0.0091, -0.0577,  0.0112, -0.0463,  0.1099, -0.0393,  0.0698,\n",
       "                      -0.0830, -0.0389], device='mps:0'))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can now be used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
